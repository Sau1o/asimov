{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13d7fc40",
   "metadata": {},
   "source": [
    "# Conceitos avançados de Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08547834",
   "metadata": {},
   "source": [
    "### Prompt few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a58a8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57675f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='10 + 3 é igual a 13.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 52, 'total_tokens': 63, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-65500778-67e3-4586-96c2-3feda5609158-0', usage_metadata={'input_tokens': 52, 'output_tokens': 11, 'total_tokens': 63, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "mensagens = [\n",
    "    HumanMessage(content='Quanto é 1 + 1?'),\n",
    "    AIMessage(content='2'),\n",
    "    HumanMessage(content='Quanto é 10 * 5?'),\n",
    "    AIMessage(content='50'),\n",
    "    HumanMessage(content='Quanto é 10 + 3?'),\n",
    "]\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5028fbaa",
   "metadata": {},
   "source": [
    "Isto é similar a formação de mensagens da api da OpenAI, mas com uma sintaxe diferente:\n",
    "\n",
    "```python\n",
    "mensagens = [\n",
    "    {'role': 'user', 'content': 'Quanto é 1 + 1'},\n",
    "    {'role': 'assistant', 'content': '2'},\n",
    "    {'role': 'user', 'content': 'Quanto é 10 * 5'},\n",
    "    {'role': 'assistant', 'content': '50'},\n",
    "    {'role': 'user', 'content': 'Quanto é 10 + 3'},\n",
    "]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45629a5c",
   "metadata": {},
   "source": [
    "## Utilizando outros modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7401c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.chat_models.huggingface import ChatHuggingFace  # Atualizando importação!\n",
    "from langchain_huggingface.llms.huggingface_endpoint import HuggingFaceEndpoint # Atualizando importação!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9915e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1211548bd58f4f0aafd4df84c16db243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9739f737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4604831150e845f5ab4a505541bf284d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f6e48ade07494fbbc05fb3061c65de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d40a99b16541bf93f56f1b66bb7d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee8d61fb9bd465195e3df21dd9b386e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelo = 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
    "\n",
    "llm = HuggingFaceEndpoint(repo_id=modelo)\n",
    "chat = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68ce7a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\" 13. To determine the sum of two numbers, you can follow these steps:\\n\\n1. Identify the two numbers you want to add (in this case, 10 and 3)\\n2. Write the numbers down, one above the other, with the smaller number on top and the larger number on the bottom.\\n3. Starting from the right, add the digits in the ones place (in this case, 0 + 3 = 3)\\n4. If the sum is greater than 9, carry the extra digit over to the next place value. In this case, the sum is only 3, so there is no need to carry anything over.\\n5. Move to the next place value to the left, and add the digits in this place value, including the carried digit (if any) from the previous step. In this case, there are no digits in the tens place, so the sum is simply 10.\\n6. The final sum is the total of the sums in each place value. In this case, the sum is 13 (10 in the tens place and 3 in the ones place).\\n\\nI hope this helps to clarify the process for adding numbers! If you have any more questions, don't hesitate to ask.\", additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=277, prompt_tokens=58, total_tokens=335), 'model': '', 'finish_reason': 'stop'}, id='run-450e1294-c7b4-4362-a795-121c834997f4-0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "mensagens = [\n",
    "    HumanMessage(content='Quanto é 1 + 1?'),\n",
    "    AIMessage(content='2'),\n",
    "    HumanMessage(content='Quanto é 10 * 5?'),\n",
    "    AIMessage(content='50'),\n",
    "    HumanMessage(content='Quanto é 10 + 3?'),\n",
    "]\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b62f3ef",
   "metadata": {},
   "source": [
    "A estrutura de chat_model utiliza a estrutura de llm como backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa47ffe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatHuggingFace] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Quanto é 1 + 1?\\nAI: 2\\nHuman: Quanto é 10 * 5?\\nAI: 50\\nHuman: Quanto é 10 + 3?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatHuggingFace] [373ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" 13. To determine the sum of two numbers, you can follow these steps:\\n\\n1. Identify the two numbers you want to add (in this case, 10 and 3)\\n2. Write the numbers down, one above the other, with the smaller number on top and the larger number on the bottom.\\n3. Starting from the right, add the digits in the ones place (in this case, 0 + 3 = 3)\\n4. If the sum is greater than 9, carry the extra digit over to the next place value. In this case, the sum is only 3, so there is no need to carry anything over.\\n5. Move to the next place value to the left, and add the digits in this place value, including the carried digit (if any) from the previous step. In this case, there are no digits in the tens place, so the sum is simply 10.\\n6. The final sum is the total of the sums in each place value. In this case, the sum is 13 (10 in the tens place and 3 in the ones place).\\n\\nI hope this helps to clarify the process for adding numbers! If you have any more questions, don't hesitate to ask.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \" 13. To determine the sum of two numbers, you can follow these steps:\\n\\n1. Identify the two numbers you want to add (in this case, 10 and 3)\\n2. Write the numbers down, one above the other, with the smaller number on top and the larger number on the bottom.\\n3. Starting from the right, add the digits in the ones place (in this case, 0 + 3 = 3)\\n4. If the sum is greater than 9, carry the extra digit over to the next place value. In this case, the sum is only 3, so there is no need to carry anything over.\\n5. Move to the next place value to the left, and add the digits in this place value, including the carried digit (if any) from the previous step. In this case, there are no digits in the tens place, so the sum is simply 10.\\n6. The final sum is the total of the sums in each place value. In this case, the sum is 13 (10 in the tens place and 3 in the ones place).\\n\\nI hope this helps to clarify the process for adding numbers! If you have any more questions, don't hesitate to ask.\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 277,\n",
      "                \"prompt_tokens\": 58,\n",
      "                \"total_tokens\": 335\n",
      "              },\n",
      "              \"model\": \"\",\n",
      "              \"finish_reason\": \"stop\"\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-eff5532d-9c05-462d-a990-a0ac2078c011-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 277,\n",
      "      \"prompt_tokens\": 58,\n",
      "      \"total_tokens\": 335\n",
      "    },\n",
      "    \"model\": \"\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.debug = True\n",
    "chat.invoke(mensagens)\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90cc552",
   "metadata": {},
   "source": [
    "> Atenção, nas versões mais atuais de langchain é recomendado utilizar o método set_debug para ativar o modo de debug, da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eacc2425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatHuggingFace] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Quanto é 1 + 1?\\nAI: 2\\nHuman: Quanto é 10 * 5?\\nAI: 50\\nHuman: Quanto é 10 + 3?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatHuggingFace] [780ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" 13\\\\*\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \" 13\\\\*\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 6,\n",
      "                \"prompt_tokens\": 58,\n",
      "                \"total_tokens\": 64\n",
      "              },\n",
      "              \"model\": \"\",\n",
      "              \"finish_reason\": \"stop\"\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-4f6bcf04-5863-4c6c-8ce9-e197841d9620-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 6,\n",
      "      \"prompt_tokens\": 58,\n",
      "      \"total_tokens\": 64\n",
      "    },\n",
      "    \"model\": \"\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(True)\n",
    "chat.invoke(mensagens)\n",
    "set_debug(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa77160b",
   "metadata": {},
   "source": [
    "Outros modelos disponíveis:\n",
    "\n",
    "https://python.langchain.com/docs/integrations/chat/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1185ea9c",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a82a5eb",
   "metadata": {},
   "source": [
    "### Cache em memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b949db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat = ChatOpenAI(model='gpt-3.5-turbo-0125')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f64eae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "mensagens = [\n",
    "    SystemMessage(content='Você é um assistente engraçado.'),\n",
    "    HumanMessage(content='Quanto é 1 + 1?')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aeda5d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import InMemoryCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "751f63b9",
   "metadata": {},
   "source": [
    "Rodandando a primeira vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "209a9230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 89 ms, sys: 5.9 ms, total: 94.9 ms\n",
      "Wall time: 2.53 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Depende, em que planeta estamos fazendo essa operação? Brincadeira, a resposta é 2! Ou será que estou apenas dizendo isso para confundir você? 😉', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 30, 'total_tokens': 72, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c3c778cd-4189-4c89-a9f0-5d7d12880d67-0', usage_metadata={'input_tokens': 30, 'output_tokens': 42, 'total_tokens': 72, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1eb09f4d",
   "metadata": {},
   "source": [
    "Rodando novamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14ac4538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 473 μs, sys: 70 μs, total: 543 μs\n",
      "Wall time: 536 μs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Depende, em que planeta estamos fazendo essa operação? Brincadeira, a resposta é 2! Ou será que estou apenas dizendo isso para confundir você? 😉', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 30, 'total_tokens': 72, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c3c778cd-4189-4c89-a9f0-5d7d12880d67-0', usage_metadata={'input_tokens': 30, 'output_tokens': 42, 'total_tokens': 72, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "768b2913",
   "metadata": {},
   "source": [
    "### Cache SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77e6ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "set_llm_cache(SQLiteCache(database_path='arquivos/lancgchain_cache_db.sqlite'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "515bc3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.5 ms, sys: 131 μs, total: 21.7 ms\n",
      "Wall time: 716 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Depende, você quer a resposta matemática correta ou a resposta que vai te fazer rir?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 30, 'total_tokens': 53, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ed74a411-4d7c-451a-b150-8169996bd3bb-0', usage_metadata={'input_tokens': 30, 'output_tokens': 23, 'total_tokens': 53, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cd25599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.7 ms, sys: 11.6 ms, total: 67.4 ms\n",
      "Wall time: 67.1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Depende, você quer a resposta matemática correta ou a resposta que vai te fazer rir?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 30, 'total_tokens': 53, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ed74a411-4d7c-451a-b150-8169996bd3bb-0', usage_metadata={'input_tokens': 30, 'output_tokens': 23, 'total_tokens': 53, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cbc0b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.28 ms, sys: 0 ns, total: 3.28 ms\n",
      "Wall time: 3.01 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Depende, você quer a resposta matemática correta ou a resposta que vai te fazer rir?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 30, 'total_tokens': 53, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ed74a411-4d7c-451a-b150-8169996bd3bb-0', usage_metadata={'input_tokens': 30, 'output_tokens': 23, 'total_tokens': 53, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14444171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
