{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13d7fc40",
   "metadata": {},
   "source": [
    "# Conceitos avan√ßados de Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08547834",
   "metadata": {},
   "source": [
    "### Prompt few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a58a8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57675f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='10 + 3 √© igual a 13.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 52, 'total_tokens': 63, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-65500778-67e3-4586-96c2-3feda5609158-0', usage_metadata={'input_tokens': 52, 'output_tokens': 11, 'total_tokens': 63, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "mensagens = [\n",
    "    HumanMessage(content='Quanto √© 1 + 1?'),\n",
    "    AIMessage(content='2'),\n",
    "    HumanMessage(content='Quanto √© 10 * 5?'),\n",
    "    AIMessage(content='50'),\n",
    "    HumanMessage(content='Quanto √© 10 + 3?'),\n",
    "]\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5028fbaa",
   "metadata": {},
   "source": [
    "Isto √© similar a forma√ß√£o de mensagens da api da OpenAI, mas com uma sintaxe diferente:\n",
    "\n",
    "```python\n",
    "mensagens = [\n",
    "    {'role': 'user', 'content': 'Quanto √© 1 + 1'},\n",
    "    {'role': 'assistant', 'content': '2'},\n",
    "    {'role': 'user', 'content': 'Quanto √© 10 * 5'},\n",
    "    {'role': 'assistant', 'content': '50'},\n",
    "    {'role': 'user', 'content': 'Quanto √© 10 + 3'},\n",
    "]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45629a5c",
   "metadata": {},
   "source": [
    "## Utilizando outros modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7401c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.chat_models.huggingface import ChatHuggingFace  # Atualizando importa√ß√£o!\n",
    "from langchain_huggingface.llms.huggingface_endpoint import HuggingFaceEndpoint # Atualizando importa√ß√£o!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9915e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1211548bd58f4f0aafd4df84c16db243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9739f737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4604831150e845f5ab4a505541bf284d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f6e48ade07494fbbc05fb3061c65de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d40a99b16541bf93f56f1b66bb7d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee8d61fb9bd465195e3df21dd9b386e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelo = 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
    "\n",
    "llm = HuggingFaceEndpoint(repo_id=modelo)\n",
    "chat = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68ce7a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\" 13. To determine the sum of two numbers, you can follow these steps:\\n\\n1. Identify the two numbers you want to add (in this case, 10 and 3)\\n2. Write the numbers down, one above the other, with the smaller number on top and the larger number on the bottom.\\n3. Starting from the right, add the digits in the ones place (in this case, 0 + 3 = 3)\\n4. If the sum is greater than 9, carry the extra digit over to the next place value. In this case, the sum is only 3, so there is no need to carry anything over.\\n5. Move to the next place value to the left, and add the digits in this place value, including the carried digit (if any) from the previous step. In this case, there are no digits in the tens place, so the sum is simply 10.\\n6. The final sum is the total of the sums in each place value. In this case, the sum is 13 (10 in the tens place and 3 in the ones place).\\n\\nI hope this helps to clarify the process for adding numbers! If you have any more questions, don't hesitate to ask.\", additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=277, prompt_tokens=58, total_tokens=335), 'model': '', 'finish_reason': 'stop'}, id='run-450e1294-c7b4-4362-a795-121c834997f4-0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "mensagens = [\n",
    "    HumanMessage(content='Quanto √© 1 + 1?'),\n",
    "    AIMessage(content='2'),\n",
    "    HumanMessage(content='Quanto √© 10 * 5?'),\n",
    "    AIMessage(content='50'),\n",
    "    HumanMessage(content='Quanto √© 10 + 3?'),\n",
    "]\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b62f3ef",
   "metadata": {},
   "source": [
    "A estrutura de chat_model utiliza a estrutura de llm como backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa47ffe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatHuggingFace] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Quanto √© 1 + 1?\\nAI: 2\\nHuman: Quanto √© 10 * 5?\\nAI: 50\\nHuman: Quanto √© 10 + 3?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatHuggingFace] [373ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" 13. To determine the sum of two numbers, you can follow these steps:\\n\\n1. Identify the two numbers you want to add (in this case, 10 and 3)\\n2. Write the numbers down, one above the other, with the smaller number on top and the larger number on the bottom.\\n3. Starting from the right, add the digits in the ones place (in this case, 0 + 3 = 3)\\n4. If the sum is greater than 9, carry the extra digit over to the next place value. In this case, the sum is only 3, so there is no need to carry anything over.\\n5. Move to the next place value to the left, and add the digits in this place value, including the carried digit (if any) from the previous step. In this case, there are no digits in the tens place, so the sum is simply 10.\\n6. The final sum is the total of the sums in each place value. In this case, the sum is 13 (10 in the tens place and 3 in the ones place).\\n\\nI hope this helps to clarify the process for adding numbers! If you have any more questions, don't hesitate to ask.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \" 13. To determine the sum of two numbers, you can follow these steps:\\n\\n1. Identify the two numbers you want to add (in this case, 10 and 3)\\n2. Write the numbers down, one above the other, with the smaller number on top and the larger number on the bottom.\\n3. Starting from the right, add the digits in the ones place (in this case, 0 + 3 = 3)\\n4. If the sum is greater than 9, carry the extra digit over to the next place value. In this case, the sum is only 3, so there is no need to carry anything over.\\n5. Move to the next place value to the left, and add the digits in this place value, including the carried digit (if any) from the previous step. In this case, there are no digits in the tens place, so the sum is simply 10.\\n6. The final sum is the total of the sums in each place value. In this case, the sum is 13 (10 in the tens place and 3 in the ones place).\\n\\nI hope this helps to clarify the process for adding numbers! If you have any more questions, don't hesitate to ask.\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 277,\n",
      "                \"prompt_tokens\": 58,\n",
      "                \"total_tokens\": 335\n",
      "              },\n",
      "              \"model\": \"\",\n",
      "              \"finish_reason\": \"stop\"\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-eff5532d-9c05-462d-a990-a0ac2078c011-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 277,\n",
      "      \"prompt_tokens\": 58,\n",
      "      \"total_tokens\": 335\n",
      "    },\n",
      "    \"model\": \"\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.debug = True\n",
    "chat.invoke(mensagens)\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90cc552",
   "metadata": {},
   "source": [
    "> Aten√ß√£o, nas vers√µes mais atuais de langchain √© recomendado utilizar o m√©todo set_debug para ativar o modo de debug, da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eacc2425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatHuggingFace] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Quanto √© 1 + 1?\\nAI: 2\\nHuman: Quanto √© 10 * 5?\\nAI: 50\\nHuman: Quanto √© 10 + 3?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatHuggingFace] [780ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" 13\\\\*\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \" 13\\\\*\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 6,\n",
      "                \"prompt_tokens\": 58,\n",
      "                \"total_tokens\": 64\n",
      "              },\n",
      "              \"model\": \"\",\n",
      "              \"finish_reason\": \"stop\"\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-4f6bcf04-5863-4c6c-8ce9-e197841d9620-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 6,\n",
      "      \"prompt_tokens\": 58,\n",
      "      \"total_tokens\": 64\n",
      "    },\n",
      "    \"model\": \"\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(True)\n",
    "chat.invoke(mensagens)\n",
    "set_debug(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa77160b",
   "metadata": {},
   "source": [
    "Outros modelos dispon√≠veis:\n",
    "\n",
    "https://python.langchain.com/docs/integrations/chat/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1185ea9c",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a82a5eb",
   "metadata": {},
   "source": [
    "### Cache em mem√≥ria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b949db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat = ChatOpenAI(model='gpt-3.5-turbo-0125')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f64eae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "mensagens = [\n",
    "    SystemMessage(content='Voc√™ √© um assistente engra√ßado.'),\n",
    "    HumanMessage(content='Quanto √© 1 + 1?')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aeda5d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import InMemoryCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "751f63b9",
   "metadata": {},
   "source": [
    "Rodandando a primeira vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "209a9230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 89 ms, sys: 5.9 ms, total: 94.9 ms\n",
      "Wall time: 2.53 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Depende, em que planeta estamos fazendo essa opera√ß√£o? Brincadeira, a resposta √© 2! Ou ser√° que estou apenas dizendo isso para confundir voc√™? üòâ', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 30, 'total_tokens': 72, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c3c778cd-4189-4c89-a9f0-5d7d12880d67-0', usage_metadata={'input_tokens': 30, 'output_tokens': 42, 'total_tokens': 72, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1eb09f4d",
   "metadata": {},
   "source": [
    "Rodando novamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14ac4538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 473 Œºs, sys: 70 Œºs, total: 543 Œºs\n",
      "Wall time: 536 Œºs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Depende, em que planeta estamos fazendo essa opera√ß√£o? Brincadeira, a resposta √© 2! Ou ser√° que estou apenas dizendo isso para confundir voc√™? üòâ', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 30, 'total_tokens': 72, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c3c778cd-4189-4c89-a9f0-5d7d12880d67-0', usage_metadata={'input_tokens': 30, 'output_tokens': 42, 'total_tokens': 72, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "768b2913",
   "metadata": {},
   "source": [
    "### Cache SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77e6ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "set_llm_cache(SQLiteCache(database_path='arquivos/lancgchain_cache_db.sqlite'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "515bc3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.5 ms, sys: 131 Œºs, total: 21.7 ms\n",
      "Wall time: 716 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Depende, voc√™ quer a resposta matem√°tica correta ou a resposta que vai te fazer rir?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 30, 'total_tokens': 53, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ed74a411-4d7c-451a-b150-8169996bd3bb-0', usage_metadata={'input_tokens': 30, 'output_tokens': 23, 'total_tokens': 53, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cd25599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.7 ms, sys: 11.6 ms, total: 67.4 ms\n",
      "Wall time: 67.1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Depende, voc√™ quer a resposta matem√°tica correta ou a resposta que vai te fazer rir?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 30, 'total_tokens': 53, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ed74a411-4d7c-451a-b150-8169996bd3bb-0', usage_metadata={'input_tokens': 30, 'output_tokens': 23, 'total_tokens': 53, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cbc0b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.28 ms, sys: 0 ns, total: 3.28 ms\n",
      "Wall time: 3.01 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Depende, voc√™ quer a resposta matem√°tica correta ou a resposta que vai te fazer rir?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 30, 'total_tokens': 53, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ed74a411-4d7c-451a-b150-8169996bd3bb-0', usage_metadata={'input_tokens': 30, 'output_tokens': 23, 'total_tokens': 53, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14444171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
